# Machine Learning Review





### Logistic Regression

See this [reference](https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11): 

An example of Logistic model:

![{\displaystyle \ell =\log _{b}{\frac {p}{1-p}}=\beta _{0}+\beta _{1}x_{1}+\beta _{2}x_{2}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/4a5e86f014eb1f0744e280eb0d68485cb8c0a6c3)

For more general consideration:

![img](https://miro.medium.com/max/2000/1*WP6xNUvfdtHgjcjnOnQETw.png)

![img](https://miro.medium.com/max/1712/1*P1Wu65ic5sK8Jhq8Sl-WxQ.png)

![img](https://miro.medium.com/max/2404/1*LFUX3uWdiZ-UTc5Vl4RVjA.png)





**Cost Function**

![img](https://miro.medium.com/max/2000/1*PT9WfxoXFuE-2yiYrEu9Zg.png)

Or written as:

![img](https://miro.medium.com/max/1400/1*dEZxrHeNGlhfNt-JyRLpig.png)

**L1 and L2 Regularization**

![img](https://miro.medium.com/max/1400/1*vwhvjVQiEgLcssUPX6vxig.png)

* L1: Lasso

* L2: Ridge



**Gradient Descent**

![img](https://i.stack.imgur.com/zgdnk.png)



### Gradient Descent and other Methods

See this [reference](https://medium.com/@lachlanmiller_52885/machine-learning-week-1-cost-function-gradient-descent-and-univariate-linear-regression-8f5fe69815fd):

![img](https://miro.medium.com/max/1400/1*WGHn1L4NveQ85nn3o7Dd2g.png)

![img](https://miro.medium.com/max/1020/1*QKHtyn4Rr-0R-s0an1eSsA.png)





### Overfitting vs Underfitting







